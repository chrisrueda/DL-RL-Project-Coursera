# Sentiment Analysis with Deep Learning (RNN and Transformer-BERT)

# Importing Libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.utils import to_categorical
from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow as tf

# Load Dataset
data = pd.read_csv('amazon_reviews.csv')  # dataset with columns ['review', 'sentiment']

# Prepare data
X = data['review'].values
y = pd.get_dummies(data['sentiment']).values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Tokenization and padding for RNN models
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=100)
X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=100)

# Simple LSTM Model
model_lstm = Sequential([
    Embedding(10000, 64, input_length=100),
    LSTM(64),
    Dense(3, activation='softmax')
])

model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_lstm.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_split=0.2)

# Evaluate LSTM
lstm_preds = model_lstm.predict(X_test_seq).argmax(axis=1)
print("LSTM Model Classification Report:")
print(classification_report(y_test.argmax(axis=1), lstm_preds))

# Bidirectional LSTM
model_bilstm = Sequential([
    Embedding(10000, 64, input_length=100),
    Bidirectional(LSTM(64)),
    Dense(3, activation='softmax')
])

model_bilstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_bilstm.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_split=0.2)

# Evaluate BiLSTM
bilstm_preds = model_bilstm.predict(X_test_seq).argmax(axis=1)
print("Bidirectional LSTM Classification Report:")
print(classification_report(y_test.argmax(axis=1), bilstm_preds))

# Transformer Model (BERT)
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
X_train_enc = bert_tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')
X_test_enc = bert_tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')

model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
model_bert.compile(optimizer=optimizer, loss=model_bert.compute_loss, metrics=['accuracy'])

model_bert.fit(
    {'input_ids': X_train_enc['input_ids'], 'attention_mask': X_train_enc['attention_mask']},
    y_train,
    epochs=2,
    batch_size=16,
    validation_split=0.1
)

# Evaluate BERT
bert_preds = model_bert.predict({'input_ids': X_test_enc['input_ids'], 'attention_mask': X_test_enc['attention_mask']})
bert_preds_labels = bert_preds.logits.numpy().argmax(axis=1)

print("BERT Classification Report:")
print(classification_report(y_test.argmax(axis=1), bert_preds_labels))
